mlr3resampling provides new cross-validation algorithms for the mlr3
framework in R

| [[file:tests/testthat][tests]]    | [[https://github.com/tdhock/mlr3resampling/actions][https://github.com/tdhock/mlr3resampling/workflows/R-CMD-check/badge.svg]] |
| [[https://github.com/jimhester/covr][coverage]] | [[https://app.codecov.io/gh/tdhock/mlr3resampling?branch=main][https://codecov.io/gh/tdhock/mlr3resampling/branch/main/graph/badge.svg]]  |

** Installation

#+begin_src R
  install.packages("mlr3resampling")#release version from CRAN
  ## OR: development version from GitHub:
  install.packages("remotes")
  remotes::install_github("tdhock/mlr3resampling")
#+end_src

** Description

*** Algorithm 1: cross-validation for comparing train on same and other

See examples in [[https://cloud.r-project.org/web/packages/mlr3resampling/vignettes/ResamplingSameOtherCV.html][vignette]] and data viz for [[https://tdhock.github.io/2023-12-13-train-predict-subsets-regression/][regression]] and [[https://tdhock.github.io/2023-12-13-train-predict-subsets-classification/][classification]].

A supervised learning algorithm inputs a train set, and outputs a
prediction function, which can be used on a test set. If each data
point belongs to a group (such as geographic region, year, etc), then
how do we know if it is possible to train on one group, and predict
accurately on another group? Cross-validation can be used to determine
the extent to which this is possible, by first assigning fold IDs from
1 to K to all data (possibly using stratification, usually by group
and label). Then we loop over test sets (group/fold combinations),
train sets (same group, other groups, all groups), and compute
test/prediction accuracy for each combination.  Comparing
test/prediction accuracy between same and other, we can determine the
extent to which it is possible (perfect if same/other have similar
test accuracy for each group; other is usually somewhat less accurate
than same; other can be just as bad as featureless baseline when the
groups have different patterns). This is implemented in

#+begin_src R
> mlr3resampling::ResamplingSameOtherCV$new()
<ResamplingSameOtherCV> : Same versus Other Cross-Validation
* Iterations:
* Instantiated: FALSE
* Parameters:
List of 1
 $ folds: int 3
#+end_src

*** Algorithm 2: cross-validation for comparing different sized train sets

See examples in [[https://cloud.r-project.org/web/packages/mlr3resampling/vignettes/ResamplingVariableSizeTrainCV.html][vignette]] and data viz for [[https://tdhock.github.io/2023-12-26-train-sizes-regression/][regression]] and [[https://tdhock.github.io/2023-12-27-train-sizes-classification/][classification]].

How many train samples are required to get accurate predictions on a
test set? Cross-validation can be used to answer this question, with
variable size train sets. This is implemented in

#+begin_src R
> mlr3resampling::ResamplingVariableSizeTrainCV$new()
<ResamplingVariableSizeTrainCV> : Cross-Validation with variable size train sets
* Iterations:
* Instantiated: FALSE
* Parameters:
List of 4
 $ folds         : int 3
 $ min_train_data: int 10
 $ random_seeds  : int 3
 $ train_sizes   : int 5
#+end_src

*** More Usage Examples and Discussion

The examples linked below have examples with larger data sizes than
the examples in the CRAN vignettes linked above.

- https://tdhock.github.io/blog/2023/R-gen-new-subsets/
- https://tdhock.github.io/blog/2023/variable-size-train/

** Related work

mlr3resampling code was copied/modified from Resampling and
ResamplingCV classes in the excellent [[https://github.com/mlr-org/mlr3][mlr3]] package.
