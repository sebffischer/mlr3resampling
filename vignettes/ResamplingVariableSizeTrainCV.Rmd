---
title: "Comparing train set sizes"
author: "Toby Dylan Hocking"
vignette: >
  %\VignetteIndexEntry{Comparing train set sizes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style type="text/css">
.main-container {
  max-width: 1200px !important;
  margin: auto;
}
</style>

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  fig.width=6,
  fig.height=6)
data.table::setDTthreads(1)
## output: rmarkdown::html_vignette above creates html where figures are limited to 700px wide.
## Above CSS from https://stackoverflow.com/questions/34906002/increase-width-of-entire-html-rmarkdown-output main-container is for html_document, body is for html_vignette
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The goal of this vignette is to explain how to
`ResamplingVariableSizeTrainCV`, which can be used to determine how
many train data are necessary to provide accurate predictions on a
given test set. 

## Simulated regression problems

```{r}
N <- 100
library(data.table)
set.seed(1)
abs.x <- 10
x.vec <- runif(N, -abs.x, abs.x)
reg.pattern.list <- list(
  sin=sin,
  constant=function(x)0)
reg.task.list <- list()
reg.data.list <- list()
for(task_id in names(reg.pattern.list)){
  f <- reg.pattern.list[[task_id]]
  task.dt <- data.table(
    x=x.vec,
    y = f(x.vec)+rnorm(N,sd=0.5))
  reg.data.list[[task_id]] <- data.table(task_id, task.dt)
  reg.task.list[[task_id]] <- mlr3::TaskRegr$new(
    task_id, task.dt, target="y"
  )
}
(reg.data <- rbindlist(reg.data.list))
```

```{r}
if(require(animint2)){
  ggplot()+
    geom_point(aes(
      x, y),
      data=reg.data)+
    facet_grid(task_id ~ .)
}
```

### Visualizing instance table

In the code below, we define a K-fold cross-validation experiment.

```{r}
(reg_size_cv <- mlr3resampling::ResamplingVariableSizeTrainCV$new())
```

Below we instantiate the resampling on one of the tasks:

```{r}
reg_size_cv$instantiate(reg.task.list[["sin"]])
reg_size_cv$instance
```

Above we see the instance, 

* `iteration.dt` has one row for each train/test split,
* `id.dt` has one row for each data point.

### Benchmark: computing test error

In the code below, we define two learners to compare,

```{r}
(reg.learner.list <- list(
  if(requireNamespace("rpart"))mlr3::LearnerRegrRpart$new(),
  mlr3::LearnerRegrFeatureless$new()))
```

In the code below, we define the benchmark grid, which is all
combinations of tasks (easy and impossible), learners (rpart and
featureless), and the one resampling method.

```{r}
(reg.bench.grid <- mlr3::benchmark_grid(
  reg.task.list,
  reg.learner.list,
  reg_size_cv))
```

In the code below, we execute the benchmark experiment (in parallel
using the multisession future plan).

```{r}
if(FALSE){
  if(require(future))plan("multisession")
}
(reg.bench.result <- mlr3::benchmark(
  reg.bench.grid, store_models = TRUE))
```

The code below computes the test error for each split,

```{r}
reg.bench.score <- mlr3resampling::score(reg.bench.result)
reg.bench.score[1]
```

The code below visualizes the resulting test accuracy numbers.

```{r}
if(require(animint2)){
  ggplot()+
    scale_x_log10()+
    scale_y_log10()+
    geom_point(aes(
      train_size, regr.mse, color=algorithm),
      shape=1,
      data=reg.bench.score)+
    facet_grid(
      test.fold~task_id,
      labeller=label_both,
      scales="free")
}
```

Above we plot the test error for each fold and train set size. 
Below we summarize each train set size, by taking the mean and standard deviation over each random seed.

```{r}
reg.mean.dt <- dcast(
  reg.bench.score,
  task_id + train_size + test.fold + algorithm ~ .,
  list(mean, sd),
  value.var="regr.mse")
if(require(animint2)){
  ggplot()+
    scale_x_log10()+
    scale_y_log10()+
    geom_ribbon(aes(
      train_size,
      ymin=regr.mse_mean-regr.mse_sd,
      ymax=regr.mse_mean+regr.mse_sd,
      fill=algorithm),
      alpha=0.5,
      data=reg.mean.dt)+
    geom_line(aes(
      train_size, regr.mse_mean, color=algorithm),
      shape=1,
      data=reg.mean.dt)+
    facet_grid(
      test.fold~task_id,
      labeller=label_both,
      scales="free")
}
```

It is clear from the plot above that 

* in constant task, the featureless always has smaller or equal
  prediction error rates than rpart, which indicates that rpart
  sometimes overfits for small sample sizes.
* in sin task, more than 30 samples are required for reasonable
  prediction error rates.
  
### Interactive data viz

The code below can be used to create an interactive data visualization
which allows exploring how different functions are learned during
different splits.

```{r ResamplingVariableSizeTrainCVAnimintRegression}
grid.dt <- data.table(x=seq(-abs.x, abs.x, l=101), y=0)
grid.task <- mlr3::TaskRegr$new("grid", grid.dt, target="y")
pred.dt.list <- list()
point.dt.list <- list()
for(score.i in 1:nrow(reg.bench.score)){
  reg.bench.row <- reg.bench.score[score.i]
  task.dt <- data.table(
    reg.bench.row$task[[1]]$data(),
    reg.bench.row$resampling[[1]]$instance$id.dt)
  set.ids <- data.table(
    set.name=c("test","train")
  )[
  , data.table(row_id=reg.bench.row[[set.name]][[1]])
  , by=set.name]
  i.points <- set.ids[
    task.dt, on="row_id"
  ][
    is.na(set.name), set.name := "unused"
  ]
  point.dt.list[[score.i]] <- data.table(
    reg.bench.row[, .(task_id, iteration)],
    i.points)
  i.learner <- reg.bench.row$learner[[1]]
  pred.dt.list[[score.i]] <- data.table(
    reg.bench.row[, .(
      task_id, iteration, algorithm
    )],
    as.data.table(
      i.learner$predict(grid.task)
    )[, .(x=grid.dt$x, y=response)]
  )
}
(pred.dt <- rbindlist(pred.dt.list))
(point.dt <- rbindlist(point.dt.list))
set.colors <- c(
  train="#1B9E77",
  test="#D95F02",
  unused="white")
algo.colors <- c(
  featureless="blue",
  rpart="red")
if(require(animint2)){
  viz <- animint(
    title="Variable size train set, regression",
    pred=ggplot()+
      ggtitle("Predictions for selected train/test split")+
      theme_animint(height=400)+
      scale_fill_manual(values=set.colors)+
      geom_point(aes(
        x, y, fill=set.name),
        showSelected="iteration",
        size=3,
        shape=21,
        data=point.dt)+
      scale_size_manual(values=c(
        featureless=3,
        rpart=2))+
      scale_color_manual(values=algo.colors)+
      geom_line(aes(
        x, y,
        color=algorithm,
        size=algorithm,
        group=paste(algorithm, iteration)),
        showSelected="iteration",
        data=pred.dt)+
      facet_grid(
        task_id ~ .,
        labeller=label_both),
    err=ggplot()+
      ggtitle("Test error for each split")+
      theme(legend.position="none")+
      theme_animint(height=400)+
      scale_y_log10(
        "Mean squared error on test set")+
      scale_color_manual(values=algo.colors)+
      scale_x_log10(
        "Train set size")+
      geom_line(aes(
        train_size, regr.mse,
        group=paste(algorithm, seed),
        color=algorithm),
        clickSelects="seed",
        showSelected="algorithm",
        size=4,
        data=reg.bench.score)+
      facet_grid(
        test.fold~task_id,
        labeller=label_both,
        scales="free")+
      geom_point(aes(
        train_size, regr.mse,
        color=algorithm),
        size=5,
        stroke=3,
        fill="black",
        fill_off=NA,
        showSelected=c("algorithm","seed"),
        clickSelects="iteration",
        data=reg.bench.score),
    source="https://github.com/tdhock/mlr3resampling/blob/main/vignettes/Simulations.Rmd")
  viz
}
if(FALSE){
  animint2pages(viz, "2023-12-26-train-sizes-regression")
}
```

If you are viewing this in an installed package or on CRAN, 
then there will be no data viz on this page, 
but you can view it on:
<https://tdhock.github.io/2023-12-26-train-sizes-regression/>

## Simulated classification problems

In this section we simulate a binary classification problem.

```{r}
N <- 200
library(data.table)
(full.dt <- data.table(
  label=factor(rep(c("spam","not spam"), l=N)),
  person=rep(1:2, each=0.5*N)
)[, signal := ifelse(label=="not spam", 0, 3)][])
set.seed(1)
n.people <- length(unique(full.dt$person))
for(person.i in 1:n.people){
  use.signal.vec <- list(
    easy=rep(if(person.i==1)TRUE else FALSE, N),
    impossible=full.dt$person==person.i)
  for(task_id in names(use.signal.vec)){
    use.signal <- use.signal.vec[[task_id]]
    full.dt[
    , paste0("x",person.i,"_",task_id) := ifelse(
      use.signal, signal, 0
    )+rnorm(N)][]
  }
}
full.dt
class.task.list <- list()
for(task_id in c("easy","impossible")){
  feature.names <- grep(task_id, names(full.dt), value=TRUE)
  task.col.names <- c(feature.names, "label", "person")
  task.dt <- full.dt[, task.col.names, with=FALSE]
  class.task.list[[task_id]] <- mlr3::TaskClassif$new(
    task_id, task.dt, target="label"
  )$set_col_roles(
    "person",c("group","stratum")
  )$set_col_roles(
    "label",c("target","stratum"))
}
class.task.list
(class.learner.list <- list(
  if(requireNamespace("rpart"))mlr3::LearnerClassifRpart$new(),
  mlr3::LearnerClassifFeatureless$new()))
(size_cv <- mlr3resampling::ResamplingVariableSizeTrainCV$new())
size_cv$instantiate(class.task.list$easy)
size_cv$instance$iteration.dt
size_cv$iters
size_cv$train_set(1)
size_cv$test_set(1)
(class.bench.grid <- mlr3::benchmark_grid(
  class.task.list,
  class.learner.list,
  size_cv))
if(FALSE){
  if(require(future))plan("multisession")
}
(class.bench.result <- mlr3::benchmark(
  class.bench.grid, store_models = TRUE))
```

Below we compute scores (test error) for each resampling iteration,
and show the first row of the result.

```{r}
class.bench.score <- mlr3resampling::score(class.bench.result)
class.bench.score[1]
```

Finally we plot the test error values below.

```{r}
if(require(animint2)){
  ggplot()+
    geom_point(aes(
      train_size, classif.ce, color=algorithm),
      shape=1,
      data=class.bench.score)+
    facet_grid(
      task_id ~ test.fold,
      labeller=label_both,
      scales="free")+
    scale_x_log10()
}
```

It is clear from the plot above that 

* TODO
  
## Conclusion

In this vignette we have shown how to use mlr3resampling for comparing
test error of models trained on different sized train sets.

## Session info 

```{r}
sessionInfo()
```
