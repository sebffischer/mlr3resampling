<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Resampling examples}
-->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The goal of this vignette is to explain how to quantify the extent to
which it is possible to train on one data subset (say a geographic
region such as Europe), and predict on another data subset (say North
America). The ideas are similar to my previous blog posts about how to
do this in
[python](https://tdhock.github.io/blog/2022/generalization-to-new-subsets/) and
[R](https://tdhock.github.io/blog/2023/R-gen-new-subsets/).

### Simulated data

Assume there is a data set with some rows from one person, some rows
from another,

```{r}
N <- 1000
library(data.table)
(full.dt <- data.table(
  label=factor(rep(c("spam","not spam"), l=N)),
  person=rep(1:2, each=0.5*N)
)[, signal := ifelse(label=="not spam", 0, 2)][])
```

Above each row has an person ID between 1 and 2. 
We can imagine a spam filtering system, that has training data for multiple people (here just two).
Each row in the table above represents a message which has been labeled as spam or not, by one of the two people.
Can we train on one person, and accurately predict on the other person?
To do that we will need some features, which we generate/simulate below:

```{r}
set.seed(1)
n.people <- length(unique(full.dt$person))
for(person.i in 1:n.people){
  use.signal.vec <- list(
    easy=rep(if(person.i==1)TRUE else FALSE, N),
    impossible=full.dt$person==person.i)
  for(feature.type in names(use.signal.vec)){
    use.signal <- use.signal.vec[[feature.type]]
    full.dt[
    , paste0("x",person.i,"_",feature.type) := ifelse(
      use.signal, signal, 0
    )+rnorm(N)][]
  }
}
full.dt
```

There are two sets of two features:

* For easy features, one is correlated with the label (`x1_easy`), and
  one is random noise (`x2_easy`), so the algorithm just needs to
  learn to ignore the noise feature, and concentrate on the signal
  feature. That should be possible given data from either person (same
  signal in each person).
* Each impossible feature is correlated with the label (when feature
  number same as person number), or is just noise (when person number
  different from feature number). So if the algorithm has access to
  the correct person (same as test, say person 2), then it needs to
  learn to use the corresponding feature `x2_impossible`. But if
  the algorithm does not have access to that person, then the best it
  can do is same as featureless (predict most frequent class label in
  train data).
  
### Visualization

Below we reshape the data to a table which is more suitable for visualization:

```{r}
(scatter.dt <- nc::capture_melt_multiple(
  full.dt,
  column="x[12]",
  "_",
  feature.type="easy|impossible"))
```

Below we visualize the pattern for each person and feature type:

```{r}
library(ggplot2)
ggplot()+
  geom_point(aes(
    x1, x2, color=label),
    shape=1,
    data=scatter.dt)+
  facet_grid(
    person ~ feature.type,
    labeller=label_both)
```

In the plot above, it is apparent that 

* for easy features (left), the two label classes differ in x1 values
  for both people. So it should be possible/easy to train on person 1, and
  predict accurately on person 2.
* for impossible features (right), the two people have different label
  patterns. For person 1, the two label classes differ in x1 values,
  whereas for person 2, the two label classes differ in x2 values. So
  it should be impossible to train on person 1, and predict accurately
  on person 2.

### Benchmark

```{r}
same_other <- mlr3resampling::ResamplingSameOtherCV$new()
same_other$param_set$values$folds <- 3
```


