<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Resampling examples}
-->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The goal of this vignette is to explain how to quantify the extent to
which it is possible to train on one data subset (say a geographic
region such as Europe), and predict on another data subset (say North
America). The ideas are similar to my previous blog posts about how to
do this in
[python](https://tdhock.github.io/blog/2022/generalization-to-new-subsets/) and
[R](https://tdhock.github.io/blog/2023/R-gen-new-subsets/).

### Simulated data

Assume there is a data set with 1000 rows, 

```{r}
N <- 1000
library(data.table)
(full.dt <- data.table(
  label=factor(rep(c("spam","not spam"), l=N)),
  person=rep(1:2, each=0.5*N)
)[, signal := ifelse(label=="not spam", 0, 1)*person][])
```

Above each row has an person ID between 1 and 2. 
We can imagine a spam filtering system, that has training data for multiple people (here just two).
Each row in the table above represents a message which has been labeled as spam or not, by one of the two people.
Can we train on one person, and accurately predict on the other person?
To do that we will need some features, which we generate/simulate below:

```{r}
set.seed(1)
n.people <- length(unique(full.dt$person))
for(person.i in 1:n.people){
  set(full.dt, j=paste0("feature_easy_noise",person.i), value=rnorm(N))
  full.dt[, paste0("feature_impossible",person.i) := ifelse(
    person==person.i, signal, 0)+rnorm(N)]
}
set(full.dt, j="feature_easy_signal", value=rnorm(N)+full.dt$signal)
str(full.dt)
```

TODO remove image below.

There are two sets of two features:

* For easy features, four are random noise (`feature_easy_noise1`
  etc), and one is correlated with the label (`feature_easy_signal`),
  so the algorithm just needs to learn to ignore the noise features,
  and concentrate on the signal feature. That should be possible given
  data from any image (same signal in each image).
* Each impossible feature is correlated with the label (when feature
  number same as image number), or is just noise (when image number
  different from feature number). So if the algorithm has access to
  the correct image (same as test, say image 2), then it needs to
  learn to use the corresponding feature `feature_impossible2`. But if
  the algorithm does not have access to that image, then the best it
  can do is same as featureless (predict most frequent class label in
  train data).
  
The signal is stronger for larger image numbers (image number 4 is
easier to learn from than image number 1).


```{r}
same_other <- mlr3resampling::ResamplingSameOtherCV$new()
same_other$param_set$values$folds <- 3

```


